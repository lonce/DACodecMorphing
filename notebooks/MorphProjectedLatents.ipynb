{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "016cce63-8622-42e0-918c-ae0b2909b6b7",
   "metadata": {},
   "source": [
    "In this notebook, we are interpolating between sounds in the \"z\" domain - the space of the continuous latent space. The sounds are represented as a time series of latent vectors, to that means we are interpolating between different points in latent space at each point in time.  \n",
    "\n",
    "The results of this kind of interpolation in the latent space of the codec are not particularly interesting, and sound more like a cross fade (except for the amplitude variation which interpolates between the envelopes of the two sounds at each frame).  \n",
    "\n",
    "Still, it is interesting to see that the quantized space of the codec ( with (1024^9)^100 = 1024^900 = 10^1.8k is dense enough to handle this range of sounds and mixes with such ghigh fidelity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ae02cc-68a1-4cb8-a5f8-10ed9f5b41c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd\n",
    "%cd /app\n",
    "\n",
    "import dac\n",
    "from audiotools import AudioSignal\n",
    "\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import soundfile as sf\n",
    "import IPython.display as ipd\n",
    "\n",
    "#!pip install gputil\n",
    "#import GPUtil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72176f9-2480-4b55-b2c2-115116e276c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.device_count()\n",
    "print(f'memeory on cuda 0 is  {torch.cuda.get_device_properties(0).total_memory/1e9}')\n",
    "print(f'memeory on cuda 1 is  { torch.cuda.get_device_properties(1).total_memory/1e9}')\n",
    "device = torch.device(\"cuda:0\") # if the docker was started with --gpus all, then can choose here.\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41a9785-7acb-42fe-92d4-6484aede6ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163afb37-a4a6-42c5-a17b-550ff29b4a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = dac.utils.download(model_type=\"44khz\") \n",
    "\n",
    "### This model doesn't sound as good - because it was trained on different data???\n",
    "# model_path = \"/scratch/codecs/codec.pth\" # /the default model from vampnet!\n",
    "\n",
    "model = dac.DAC.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76af5bf5-5bb0-4a8c-bea8-a78672a033c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('__CUDNN VERSION:', torch.backends.cudnn.version())\n",
    "print('__Number CUDA Devices:', torch.cuda.device_count())\n",
    "print('__CUDA Device Name:',torch.cuda.get_device_name(0))\n",
    "print('__CUDA Device Total Memory [GB]:',torch.cuda.get_device_properties(0).total_memory/1e9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2edd162-6d33-47e0-a777-9e614a4f789b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device); #wanna see the model? remove the semicolon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab9eb41-d1f9-4b6a-9b7b-f2ab54d8943d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataroot=\"/scratch/dacdevdata\" \n",
    "# !ls {datadir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580143a4-4e02-4dfa-b55f-537b7c8f5b06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "datadir=dataroot+\"/44kHz/N4/PisWinAppBee_sparse_recon\"\n",
    "\n",
    "N_QUANTIZERS = 9  ## SEEMS TO HAVE NO EFFECT - I guess because it is a property of the pretrained model?\n",
    "\n",
    "snd1_wav ='/DSApplause--numClappers_exp-00.50.wav' \n",
    "#snd1_wav='/DSPistons--rate_exp-00.50.wav'\n",
    "snd2_wav = '/DSBugs--busybodyFreqFactor-00.50.wav'\n",
    "#snd2_wav ='/DSWind--strength-00.50.wav'\n",
    "\n",
    "CORTADOFACTURA=3  #cut the wavefile lengths by this amount before loading so we don't overrun GPU memory\n",
    "\n",
    "#1) LOAD A SOUND\n",
    "snd1 = AudioSignal(datadir + snd1_wav) # 2-second sound at 16kHz\n",
    "snd1 = snd1[0,0,: int(snd1.shape[2]/CORTADOFACTURA)] # cortado, otherwise the computation will bust memory\n",
    "\n",
    "#2) PUT IT ON THE GPU\n",
    "snd1.to(model.device)\n",
    "#3) PREPROCESS (make sure sr agrees with model, i guess)\n",
    "snd1_x = model.preprocess(snd1.audio_data, snd1.sample_rate)\n",
    "#4) ENCODE TO Z, C, and L\n",
    "snd1_z, snd1_codes, snd1_latents, _, _ = model.encode(snd1_x, N_QUANTIZERS) #model.encode(snd1_x, 4)\n",
    "\n",
    "snd2 = AudioSignal(datadir + snd2_wav) # 2-second sound at 16kHz\n",
    "snd2 = snd2[0,0,: int(snd2.shape[2]/CORTADOFACTURA)] # cortado, otherwise the computation will bust memory\n",
    "\n",
    "snd2.to(model.device)\n",
    "snd2_x = model.preprocess(snd2.audio_data, snd2.sample_rate)\n",
    "snd2_z, snd2_codes, snd2_latents, _, _ = model.encode(snd2_x, N_QUANTIZERS) # model.encode(snd2_x, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc09c75a-d506-4a64-9fa0-ba999a8b1eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'snd1 Audio Signal is shape {snd1.shape}')\n",
    "print(f'snd1_z shape is: {snd1_z.shape}, and snd2_z shape is: {snd2_z.shape}')\n",
    "print(f'snd1_codes shape is: {snd1_codes.shape}, and snd2_codes shape is: {snd2_codes.shape}')\n",
    "print(f'snd1_latents shape is: {snd1_latents.shape}, and snd2_latents shape is: {snd2_latents.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a1d698-a103-4b8d-9c2a-a5bf822b06e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets check a code vector to see if we are using the number specificied above\n",
    "snd1_codes[0,:,40]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9b1f49-54a8-49c6-9284-1fa1fbda50c4",
   "metadata": {},
   "source": [
    "<div style=\"height: 10px; background-color: blue;\"></div>\n",
    "This next cell is just for reporting a bug do Descript.\n",
    "It also busts the GPU memory if audio is longer than about a second, so we comment it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edd9307-f171-48c2-ae23-7da2f34dbcc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Try to code the AudioSignal (snd1_x) with diffenent number of quantizers\n",
    "# foo1_z, foo1_c, _, _, _ = model.encode(snd1_x, n_quantizers=1)\n",
    "# foo4_z, foo4_c, _, _, _ = model.encode(snd1_x, n_quantizers=4)\n",
    "# fooN_z, fooN_c, _, _, _ = model.encode(snd1_x) # expected to use all 9 codebooks\n",
    "\n",
    "# print(f' Example code slices: \\n foo1_c: {foo1_c[0,:,40]} \\n foo4_c: {foo4_c[0,:,40]} \\n fooN_c: {fooN_c[0,:,40]}\\n')\n",
    "# print(f' And how about the z vectors that we will use to decode?\\n')\n",
    "# print(f' Are foo1_z and foo4_z tensors equal? Ans: {torch.equal(foo1_z, foo4_z)}')\n",
    "# print(f' Are foo1_z and fooN_z tensors equal? Ans: {torch.equal(foo1_z, fooN_z)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba94311e-151f-4d86-a185-9e3f387639ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Project Latents Experiment!!!!!!!!!!!!!!!!!\n",
    "### Lets just do a reality check that if we \"manually\" take the 8D latents to 1024D z and then decode, \n",
    "### it should be the same as the z we got from model encode.\n",
    "\n",
    "snd2_z_from_l,_,_ = model.quantizer.from_latents(snd2_latents)\n",
    "print(f'snd2_z_from_l shape is: {snd2_z_from_l.shape}')\n",
    "torch.dist(snd2_z_from_l, snd2_z, p=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4582067-5a1a-4f37-ae47-5d5da0d3f1d7",
   "metadata": {},
   "source": [
    "### <font color='blue'> First decode both sounds a play them </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58ad0a1-1bae-4ce3-982d-c077cbc58711",
   "metadata": {},
   "outputs": [],
   "source": [
    "snd2recon = model.decode(snd2_z_from_l) #z_from_l or z from encode are the same\n",
    "\n",
    "snd2reconsignal = snd2recon[0,0,:].cpu().detach().numpy()\n",
    "plt.plot(snd2reconsignal)\n",
    "ipd.Audio(snd2reconsignal, rate=44100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a8e7d5-8f3a-4924-8d6c-6060cf152935",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7dc6cb-cba4-44ad-ac59-88765adf4578",
   "metadata": {},
   "outputs": [],
   "source": [
    "snd1recon = model.decode(snd1_z)\n",
    "\n",
    "snd1reconsignal = snd1recon[0,0,:].cpu().detach().numpy()\n",
    "plt.plot(snd1reconsignal)\n",
    "ipd.Audio(snd1reconsignal, rate=44100)\n",
    "\n",
    "#original \n",
    "#snd1signal=snd1.audio_data.cpu().detach().numpy()[0,0,:]\n",
    "#plt.plot(snd1signal)\n",
    "#ipd.Audio(snd1signal, rate=44100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4aeb9e0-c7c0-49f5-ae1d-7094dbe5a560",
   "metadata": {},
   "source": [
    "### <font color='blue'> Now do n interpolation in the *latent* space (the latent space is the \"projected\" space and has only 8 dimensions for each codebook) </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84ed628-39f3-4801-a4ac-cb82d766c5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"morph\" between two time-indexed sequences of latent variables\n",
    "# Interpoloates between i1*va+(1-i1)vb to i2*va+(1-i2)vb\n",
    "def interp(va, vb, i1, i2) : \n",
    "    assert va.shape == vb.shape, \"Tensors must have the same shape\"\n",
    "    timesteps=va.shape[2]\n",
    "    linear_values = torch.linspace(i1, i2, timesteps, device=device)\n",
    "    complementlinear_values = 1-linear_values \n",
    "\n",
    "    return linear_values * va + complementlinear_values * vb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee3ed75-146f-4ae3-8538-67c03434994d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "la=snd2_latents\n",
    "lb=snd1_latents\n",
    "linterp=interp(la, lb, .6, .4)  ##Interpolate in the 8D projected space!\n",
    "print(f'linterp.shape is {linterp.shape}') \n",
    "\n",
    "# first from low-D (8 per codebook) to 1024\n",
    "z_from_l,_,_ = model.quantizer.from_latents(linterp)\n",
    "\n",
    "#now from 1024 z space to audio\n",
    "y = model.decode(z_from_l)\n",
    "\n",
    "print(f'signal y.shape is {y.shape}') \n",
    "mix_signal = y[0,0,:].cpu().detach().numpy()\n",
    "plt.plot(mix_signal)\n",
    "ipd.Audio(mix_signal, rate=44100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78eb8686-b19d-49a3-b283-2f0d35d865c4",
   "metadata": {},
   "source": [
    "### <font color='blue'> Now rather than interpolating in latent space, swap value in the dimensions with even index. Each sound is now made up of latents where have the values come from one sound, half the values from the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872e5f1a-c6da-4f1f-b765-2008500b1908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes the latent variable at each time step, \n",
    "#and swaps their invidual values in the dimensions with an even index\n",
    "# Thus each matrix at each time step has half its values from one sound,\n",
    "# and the other half from the other. \n",
    "\n",
    "# does this in place - modifies the original matrices\n",
    "def swap_elements_for_even_n(matrix1, matrix2):\n",
    "    # Get the shape of the input matrices\n",
    "    _, n, m = matrix1.size()\n",
    "\n",
    "    # Create masks for even and odd indices along the n dimension\n",
    "    even_mask = torch.arange(n) % 2 == 1  # Indices where n is even\n",
    "    odd_mask = torch.arange(n) % 2 == 0   # Indices where n is odd\n",
    "\n",
    "    # Select elements where n is even from both matrices\n",
    "    even_elements_matrix1 = matrix1[:, even_mask, :]\n",
    "    even_elements_matrix2 = matrix2[:, even_mask, :]\n",
    "\n",
    "    # Swap the even elements between the matrices\n",
    "    matrix1[:, even_mask, :] = even_elements_matrix2\n",
    "    matrix2[:, even_mask, :] = even_elements_matrix1\n",
    "\n",
    "    return matrix1, matrix2\n",
    "\n",
    "## test\n",
    "# a= torch.rand(1, 4, 2)\n",
    "# b= torch.rand(1, 4, 2)\n",
    "# print(a)\n",
    "# print(b)\n",
    "# print('--------')\n",
    "# swap_elements_for_even_n(a, b)\n",
    "# print(a)\n",
    "# print(b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c64a7f-4391-42bd-bd3e-63932b2c2a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "la=snd2_latents\n",
    "lb=snd1_latents\n",
    "# first do a swap of values in the latent space for even-numbered dimensions\n",
    "la, lb = swap_elements_for_even_n(la, lb)\n",
    "linterp=interp(la, lb, 0, 0)  ## (0,0 plays the first sound only, 1,1 plays the second sound only)\n",
    "print(f'linterp.shape is {linterp.shape}') \n",
    "\n",
    "z_from_l,_,_ = model.quantizer.from_latents(linterp)\n",
    "y = model.decode(z_from_l)\n",
    "print(f'signal y.shape is {y.shape}') \n",
    "mix_signal = y[0,0,:].cpu().detach().numpy()\n",
    "plt.plot(mix_signal)\n",
    "ipd.Audio(mix_signal, rate=44100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f588f7-c18c-4b33-8690-eafdc5a62593",
   "metadata": {},
   "source": [
    "### <font color='blue'> Now reconstruct from the lowD latents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86f1f69-beb8-49b5-a033-9c94e4800461",
   "metadata": {},
   "outputs": [],
   "source": [
    "z,_,_ = model.quantizer.from_latents(snd1_latents)\n",
    "snd = model.decode(z) #z_from_l or z from encode are the same\n",
    "\n",
    "recon = snd[0,0,:].cpu().detach().numpy()\n",
    "plt.plot(recon)\n",
    "ipd.Audio(recon, rate=44100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e55935-5db8-411d-97d6-e46060aa0402",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GPUtil.showUtilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35494f6-b341-4797-93fe-4bb1e4e6d9d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
