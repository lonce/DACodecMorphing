{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "016cce63-8622-42e0-918c-ae0b2909b6b7",
   "metadata": {},
   "source": [
    "### <font color='blue'> In this notebook, we want to understand the cost of going from the 9*8D projected latent representation to a PNG image with its [0,255] resolution. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ae02cc-68a1-4cb8-a5f8-10ed9f5b41c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd\n",
    "%cd /app\n",
    "\n",
    "\n",
    "import dac\n",
    "from audiotools import AudioSignal\n",
    "\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import soundfile as sf\n",
    "import IPython.display as ipd\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72176f9-2480-4b55-b2c2-115116e276c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.device_count()\n",
    "torch.cuda.get_device_properties(0).total_memory/1e9\n",
    "torch.cuda.get_device_properties(1).total_memory/1e9\n",
    "device = torch.device(\"cuda:0\") # if the docker was started with --gpus all, then can choose here.\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163afb37-a4a6-42c5-a17b-550ff29b4a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = dac.utils.download(model_type=\"44khz\") \n",
    "\n",
    "### This model doesn't sound as good - because it was trained on different data???\n",
    "# model_path = \"/scratch/codecs/codec.pth\" # /the default model from vampnet!\n",
    "\n",
    "model = dac.DAC.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76af5bf5-5bb0-4a8c-bea8-a78672a033c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('__CUDNN VERSION:', torch.backends.cudnn.version())\n",
    "print('__Number CUDA Devices:', torch.cuda.device_count())\n",
    "print('__CUDA Device Name:',torch.cuda.get_device_name(0))\n",
    "print('__CUDA Device Total Memory [GB]:',torch.cuda.get_device_properties(0).total_memory/1e9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2edd162-6d33-47e0-a777-9e614a4f789b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device); #wanna see the model? remove the semicolon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab9eb41-d1f9-4b6a-9b7b-f2ab54d8943d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataroot=\"dacdevdata\" \n",
    "# !ls {datadir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580143a4-4e02-4dfa-b55f-537b7c8f5b06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "datadir=dataroot+\"/44kHz/N4/PisWinAppBee_sparse_recon\"\n",
    "\n",
    "N_QUANTIZERS = 9  ## SEEMS TO HAVE NO EFFECT - I guess because it is a property of the pretrained model?\n",
    "\n",
    "snd2_wav ='/DSApplause--numClappers_exp-00.50.wav' \n",
    "#snd2_wav='/DSPistons--rate_exp-00.50.wav'\n",
    "#snd2_wav = '/DSBugs--busybodyFreqFactor-00.50.wav'\n",
    "#snd2_wav ='/DSWind--strength-00.50.wav'\n",
    "\n",
    "CORTADOFACTURA=1  #cut the wavefile lengths by this amount before loading so we don't overrun GPU memory\n",
    "\n",
    "#1) LOAD A SOUND\n",
    "\n",
    "snd2 = AudioSignal(datadir + snd2_wav) # 2-second sound at 16kHz\n",
    "snd2 = snd2[0,0,: int(snd2.shape[2]/CORTADOFACTURA)] # cortado, otherwise the computation will bust memory\n",
    "\n",
    "snd2.to(model.device)\n",
    "snd2_x = model.preprocess(snd2.audio_data, snd2.sample_rate)\n",
    "with torch.no_grad():\n",
    "    snd2_z, snd2_codes, snd2_latents, _a, _b = model.encode(snd2_x, N_QUANTIZERS) # model.encode(snd2_x, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc09c75a-d506-4a64-9fa0-ba999a8b1eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f'snd2_z shape is: {snd2_z.shape}')\n",
    "print(f'snd2_codes shape is: {snd2_codes.shape}')\n",
    "print(f'snd2_latents shape is: {snd2_latents.shape}')\n",
    "print(f'snd2_codes at t=40 are: {snd2_codes[0,:,40]}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4950a43-ebef-4029-b597-74c6b7869fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "del snd2_codes,_a, _b, snd2_x\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4582067-5a1a-4f37-ae47-5d5da0d3f1d7",
   "metadata": {},
   "source": [
    "### <font color='blue'> First decode sound and play </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58ad0a1-1bae-4ce3-982d-c077cbc58711",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    snd2recon = model.decode(snd2_z) #z_from_l or z from encode are the same\n",
    "snd2reconsignal = snd2recon[0,0,:].cpu().detach().numpy()\n",
    "plt.plot(snd2reconsignal)\n",
    "ipd.Audio(snd2reconsignal, rate=44100)\n",
    "\n",
    "del  snd2recon\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73b5184-cddf-48b6-bb57-6fcba2ad1439",
   "metadata": {},
   "source": [
    "### <font color='blue'> Now look at the histogram of the values in the latent variables (choose one in the next cell by assigning it to examinevector)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69b541c-d14e-4cbb-964f-515052cd937a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at higher codebooks in the residual stack\n",
    "#examinevector=snd2_latents[0,32:71,:] # [-6, 6]\n",
    "\n",
    "# look at lower codebooks in the residual stack\n",
    "#examinevector=snd2_latents[0,0:31,:]   # [-10, 10]\n",
    "\n",
    "# look at all values in all codebooks\n",
    "examinevector=snd2_latents[0,:,:]   # [-10, 10]\n",
    "\n",
    "examinevector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b18694f-73e8-4613-8018-804b003fcf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "smallest_value = torch.min(examinevector)\n",
    "largest_value = torch.max(examinevector)\n",
    "print(f\"The smallest value is {smallest_value.item()}\")\n",
    "print(f\"The largest value is {largest_value.item()} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4510ddb2-5b24-460d-bf03-799b00204a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the tensor to 1D\n",
    "numpy_array = examinevector.cpu().detach().numpy().flatten()\n",
    "\n",
    "# Define the number of bins\n",
    "num_bins = 100\n",
    "\n",
    "# Create a histogram using NumPy to compute bin edges and counts\n",
    "hist, bins = np.histogram(numpy_array, bins=num_bins, range=(-12,12))\n",
    "\n",
    "\n",
    "# Plot the histogram\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.hist(bins[:-1], bins, weights=hist)\n",
    "plt.title('Histogram of 3D Tensor Values')\n",
    "plt.xlabel('Values')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Calculate positions for the labels spread across the horizontal width of the image\n",
    "# label_positionsx = np.linspace(-num_bins/2, num_bins/2, num_bins)\n",
    "# label_positionsy = linspace_v(0, np.max(hist), num_bins)\n",
    "\n",
    "# #print(label_positions)\n",
    "# # Annotate the plot with average values, rotating labels 90 degrees clockwise\n",
    "# for i in range(num_bins):\n",
    "#     label = f'{bin_avg_values[i]:.2f}'\n",
    "\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52f443e-3de9-4777-91c0-c7cc0fbabb1b",
   "metadata": {},
   "source": [
    "### <font color='blue'> Now clip the outliers and test sound quality.If it sounds good, we have our range for shifting into the PNG representation range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86f1f69-beb8-49b5-a033-9c94e4800461",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIPMIN=-5  # wow, sounds good even seriously clamping the values of the Gaussian tails!\n",
    "CLIPMAX=5\n",
    "\n",
    "#The norm is not exactly the same for all sounds, but roughly.\n",
    "print(f'norm of the projected latents is {torch.norm(snd2_latents)}')\n",
    "\n",
    "# Clamping does not degrade much (presumably because it doesn't change the angle very much)\n",
    "modlatents = torch.clamp(snd2_latents, CLIPMIN, CLIPMAX)\n",
    "\n",
    "###### Just some other random manipulation experiments \n",
    "### WOW - small effect \n",
    "#  modlatents = -modlatents\n",
    "### also modest effect \n",
    "# modlatents=torch.flip(modlatents,[2])\n",
    "## huge effect. Dimension matters\n",
    "# modlatents=shuffledimrange(modlatents.cpu(),1, 0, 7).cuda()\n",
    "\n",
    "print(f'torch.max(modlatents) is {torch.max(modlatents)}')\n",
    "\n",
    "z,_a,_b = model.quantizer.from_latents(modlatents)\n",
    "print(f'torch.max(z) is {torch.max(z)}')\n",
    "\n",
    "del _a,_b\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "with torch.no_grad():\n",
    "    snd = model.decode(z) #z_from_l or z from encode are the same\n",
    "\n",
    "recon = snd[0,0,:].cpu().detach().numpy()\n",
    "plt.plot(recon)\n",
    "ipd.Audio(recon, rate=44100)\n",
    "\n",
    "del z, snd\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb749cb6-5767-4884-a259-dc0845221e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a heatmap of the tensor just to see what it looks like\n",
    "data=modlatents.cpu().detach().numpy()[0,:,:]\n",
    "plt.figure(figsize=(8, 6))  # Adjust the figure size as needed\n",
    "plt.imshow(data, cmap='viridis', aspect='auto',origin='lower')  # 'viridis' colormap is used, 'aspect=auto' for aspect ratio\n",
    "\n",
    "# Add colorbar for reference\n",
    "plt.colorbar()\n",
    "\n",
    "# Set labels and title\n",
    "plt.xlabel('X Axis Label')\n",
    "plt.ylabel('Y Axis Label')\n",
    "plt.title('Heatmap of 2D Tensor')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e77725-1e2e-4c55-9a01-8f55cc898695",
   "metadata": {},
   "source": [
    "### <font color='blue'> Now create the gray-scale PNG. This means clipping, scaling, shifting, and descretizing to get the [0,255] we need. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d970d8-852c-4526-8f88-0ea1c42f8719",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_and_shift_to_uint8(tensor, minval, maxval):\n",
    "    # Crop values between minval and maxval\n",
    "    cropped_tensor = torch.clamp(tensor, minval, maxval)\n",
    "    \n",
    "    # Normalize the cropped tensor to range [0, 1]\n",
    "    normalized_tensor = (cropped_tensor - minval) / (maxval - minval)\n",
    "    \n",
    "    # Scale values to range [0, 255] and convert to uint8\n",
    "    uint8_tensor = (normalized_tensor * 255).to(torch.uint8)\n",
    "    \n",
    "    return uint8_tensor\n",
    "\n",
    "def shift_uint8_to_float(image_uint8, minval, maxval):\n",
    "    # Convert uint8 image tensor to float and scale it to [0, 1]\n",
    "    normalized_image = image_uint8.float() / 255.0\n",
    "    \n",
    "    # Shift and scale the values to the desired range [minval, maxval]\n",
    "    float_tensor = (normalized_image * (maxval - minval)) + minval\n",
    "    \n",
    "    return float_tensor\n",
    "\n",
    "def display_uint8_image(image_uint8):\n",
    "    # Convert uint8 tensor to a NumPy array\n",
    "    image_np = image_uint8.numpy()\n",
    "\n",
    "    # Display the image using Matplotlib\n",
    "    plt.imshow(image_np, cmap='gray', vmin=0, vmax=255,origin='lower')  # Assuming grayscale image\n",
    "    plt.axis('off')  # Hide axis\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5e82fc-aa0a-43bd-9770-43c0d67b7c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pngdata = crop_and_shift_to_uint8(snd2_latents, CLIPMIN, CLIPMAX) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0401c394-9e86-4910-acd6-c650245c673a",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_uint8_image(pngdata.cpu()[0,:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3f5f43-5608-46e8-8aff-4dbddc314c19",
   "metadata": {},
   "source": [
    "### <font color='blue'> Now go from PNG to sound to see if the clipping and descretizing did any harm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2d8a0b-b502-43ff-b0cd-8199fbffa44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructedlatents=shift_uint8_to_float(pngdata, CLIPMIN, CLIPMAX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f8d538-1522-4f81-9418-f1cf2e1b2828",
   "metadata": {},
   "outputs": [],
   "source": [
    "z,_a,_b = model.quantizer.from_latents(reconstructedlatents)\n",
    "print(f'torch.max(z) is {torch.max(z)}')\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    snd = model.decode(z) #z_from_l or z from encode are the same\n",
    "\n",
    "recon = snd[0,0,:].cpu().detach().numpy()\n",
    "plt.plot(recon)\n",
    "ipd.Audio(recon, rate=44100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e92e39f-5f17-4a30-a708-2a11d16edaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "del z, snd, snd2_latents, snd2_z, recon, _a, _b\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200181dd-8179-4bf2-bebc-6d946c9962df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
