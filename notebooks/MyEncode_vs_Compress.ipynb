{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b33233d6-25a9-4b3e-9249-295e28a67cbf",
   "metadata": {},
   "source": [
    "### For exploring the difference between manual compression/decompressiong vs command line decoding and encoding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3536ce3b-7d46-474c-aa71-f02e1a14c0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE='cpu' # either 'cuda' or 'cpu'\n",
    "\n",
    "%pwd\n",
    "%cd /app\n",
    "\n",
    "import os\n",
    "import dac\n",
    "from audiotools import AudioSignal\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import soundfile as sf\n",
    "import IPython.display as ipd\n",
    "\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486bac65-041e-494f-b49e-647110129654",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEVICE=='cuda' :\n",
    "    if torch.cuda.is_available() :\n",
    "        nd=torch.cuda.device_count()\n",
    "        for i in range(nd) : \n",
    "            torch.cuda.get_device_properties(i).total_memory/1e9\n",
    "            print(f'memeory on cuda {i} is  {torch.cuda.get_device_properties(i).total_memory/1e9}')\n",
    "        device = torch.device('cuda') # if the docker was started with --gpus all, then can choose here with cuda:0 (or cpu)\n",
    "    else :\n",
    "        device = torch.device('cuda')\n",
    "\n",
    "device = torch.device(DEVICE)\n",
    "print(f'Device is {device}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312cbf31-aaa0-424a-9c5e-558b740bd2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = dac.utils.download(model_type=\"44khz\") \n",
    "model = dac.DAC.load(model_path)\n",
    "\n",
    "model.to(device); #wanna see the model? remove the semicolon\n",
    "model.eval();  # need to be \"in eval mode\" in order to set the number of quantizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b1b0e9-e1d8-4b64-8ccb-ddd6ecfcb614",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataroot=\"dacdevdata\" \n",
    "!ls ./{dataroot}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8113df3a-e104-4b9b-9220-ce38167a91c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_QUANTIZERS=4\n",
    "\n",
    "signal_filename = dataroot+'/44kHz/N4/PisWinAppBee_sparse_recon/DSApplause--numClappers_exp-00.50.wav'\n",
    "signal_dacfilelame = dataroot+'/44kHz/N4/PisWinAppBee_sparse_dac/DSApplause--numClappers_exp-00.50.dac'\n",
    "\n",
    "with torch.no_grad():\n",
    "    signal = AudioSignal(signal_filename)\n",
    "    signal_compressed = model.compress(signal, n_quantizers=N_QUANTIZERS)  # RET: dac_file\n",
    "    signal_compressed.save(signal_dacfilelame)\n",
    "\n",
    "    # for testing and comparison:\n",
    "    signal_audio = signal.cpu().detach().numpy()[0,0,:]\n",
    "\n",
    "    signal.to(model.device)\n",
    "    signal_prep = model.preprocess(signal.audio_data, signal.sample_rate)\n",
    "    with torch.no_grad():\n",
    "        signal_encoded_z, signal_encoded_codes, signal_encoded_latents, _, _ = model.encode(signal_prep, N_QUANTIZERS) # model.encode(snd2_x, 4)\n",
    "\n",
    "    print(f'--')\n",
    "    print(f'signal_compressed.codes shape is: {signal_compressed.codes.shape}')\n",
    "    print(f'signal_encoded_codes shape is: {signal_encoded_codes.shape}') \n",
    "    print(f'--')\n",
    "#----------------------------------------\n",
    "    signal_dacfile = dac.DACFile.load(signal_dacfilelame)\n",
    "    print(f'dacfile.codes shape is: {signal_dacfile.codes.shape}')\n",
    "\n",
    "    print(f'--')\n",
    "    # Is signal_compressed the same as signal_dacfile?\n",
    "    print(f' signal_compressed.codes == signal_dacfile.codes? ..... {signal_compressed.codes == signal_dacfile.codes}')\n",
    "    print(f' signal_compressed.chunk_length == signal_dacfile.chunk_length? ..... {signal_compressed.chunk_length == signal_dacfile.chunk_length}')\n",
    "    print(f' signal_compressed.sample_rate == signal_dacfile.sample_rate? ..... {signal_compressed.sample_rate == signal_dacfile.sample_rate}')\n",
    "    print(f' signal_compressed.padding == signal_dacfile.padding? ..... {signal_compressed.padding == signal_dacfile.padding}')\n",
    "    print(f' signal_compressed.channels == signal_dacfile.channels? ..... {signal_compressed.channels == signal_dacfile.channels}')\n",
    "    print(f' signal_compressed.original_length == signal_dacfile.original_length? ..... {signal_compressed.original_length == signal_dacfile.original_length}')\n",
    "    print(f'------------------')\n",
    "#-----------------------------------------\n",
    "    #First decompress dacfile\n",
    "    signal_decompressed = model.decompress(signal_dacfile, verbose=False)  #an AudioSignal\n",
    "    audio_decompressed = signal_decompressed.cpu().detach().numpy()[0,0,:]\n",
    "\n",
    "    #Now decode dacfile\n",
    "    z_from_c, l_from_c,c =model.quantizer.from_codes(signal_dacfile.codes.to(device))\n",
    "    print(f'z_from_c shape is: {z_from_c.shape}')\n",
    "    print(f'l_from_c shape is: {l_from_c.shape}')\n",
    "    print(f'c shape is: {c.shape}')\n",
    "\n",
    "    zq, zp, c =model.quantizer.from_latents(l_from_c)\n",
    "    print(f'z_from_c is {z_from_c}')\n",
    "    print(f'zq is {zq}')\n",
    "    signal_decode_tensor = model.decode(zq) ####  (z_from_c) ### \n",
    "\n",
    "    \n",
    "    audio_decoded = signal_decode_tensor.cpu().detach().numpy()[0,0,:]\n",
    "\n",
    "    print(f'------------------')\n",
    "    \n",
    "#-------------------------------\n",
    "    print(f'ORIGINAL AudioSignal has length = {str(len(signal_audio))}')\n",
    "    print(f'Mannually decoded AudioSignal has length = {str(len(audio_decoded))}')\n",
    "    print(f'DECOMPRESSED AudioSignal has length = {str(len(audio_decompressed))}')\n",
    "    \n",
    "\n",
    "    print(f'------------------')\n",
    "    print(f' --------  compare to embeddings from original signal    --------')\n",
    "    print(f' signal_encoded_z shape is: { signal_encoded_z.shape}')\n",
    "    print(f'signal_encoded_latents shape is: {signal_encoded_latents.shape}')\n",
    "    print(f'signal_encoded_codes shape is: {signal_encoded_codes.shape}')   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b6c1bb-c1a1-4729-a52d-f459c35c0cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "# load the file, and then \n",
    "with torch.no_grad():\n",
    "    dacfile = dac.DACFile.load(dataroot+'/44kHz/N4/PisWinAppBee_sparse_dac/DSApplause--numClappers_exp-00.50.dac')\n",
    "    \n",
    "    # FIRST - Decompress it back to an AudioSignal\\ from codes to z (1024) to signal   \n",
    "    print(f'dacfile.codes shape is: {dacfile.codes.shape}')\n",
    "    z_from_c, l_from_c,c =model.quantizer.from_codes(dacfile.codes.to(device))\n",
    "    print(f'z_from_c shape is: {z_from_c.shape}')\n",
    "    print(f'l_from_c shape is: {l_from_c.shape}')\n",
    "    print(f'c shape is: {c.shape}')\n",
    "    xTensor = model.decode(z_from_c)\n",
    "\n",
    "    # SECOND - Decompress directly using model.decompress   \n",
    "    yAudioSig = model.decompress(dacfile)  #an AudioSignal\n",
    "\n",
    "    \n",
    "print(f' The length of  xTensor  signal is {len(xTensor.cpu().detach().numpy()[0,0,:])}')\n",
    "print(f' The length of yAudioSig signal is {len(yAudioSig.cpu().detach().numpy()[0,0,:])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eead438-ac4a-45ce-b34b-9ebebdec5288",
   "metadata": {},
   "outputs": [],
   "source": [
    "xsig=xTensor.cpu().detach().numpy()[0,0,:]\n",
    "plt.plot(xsig)\n",
    "ipd.Audio(xsig, rate=44100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cc109a-6d53-4f59-8047-d2c6254eced8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "ysig=yAudioSig.cpu().detach().numpy()[0,0,:]\n",
    "plt.plot(ysig)\n",
    "ipd.Audio(ysig, rate=44100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e87e5b-11a1-4f0c-bc3b-d920eccd8740",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(xsig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009136d2-a8f1-469a-97db-872eef693e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ysig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09516847-2be9-409f-91f3-df542a0c7c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_arrays(arr1, arr2):\n",
    "    \"\"\"\n",
    "    Takes two arrays, pads the shorter one with zeros at the end to make them of equal length,\n",
    "    and plots both on a Matplotlib graph with different colors.\n",
    "\n",
    "    Args:\n",
    "    arr1 (np.array): First input array.\n",
    "    arr2 (np.array): Second input array.\n",
    "    \"\"\"\n",
    "    # Determine the maximum length of the two arrays\n",
    "    max_length = max(len(arr1), len(arr2))\n",
    "\n",
    "    # Pad arrays to the maximum length\n",
    "    padded_arr1 = np.pad(arr1, (0, max_length - len(arr1)), mode='constant')\n",
    "    padded_arr2 = np.pad(arr2, (0, max_length - len(arr2)), mode='constant')\n",
    "\n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(padded_arr1, label='Array 1', color='blue')\n",
    "    plt.plot(padded_arr2, label='Array 2', color='green')\n",
    "    plt.title('Comparison of Two Arrays')\n",
    "    plt.xlabel('Index')\n",
    "    plt.ylabel('Value')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f71ce01-9fda-4a08-a9ba-b9a21abfa0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_arrays(xsig[8000+4096:(25000+4096)], ysig[8000:35000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88365e42-e49e-4bfa-ad66-b5621d81bac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_arrays(xsig[25000+4096:(50000+4096)], ysig[25000:50000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26590515-df7c-4b09-8f17-83bb110a8721",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_arrays(xsig, ysig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db641394-9afc-4d20-8401-9d15f366acf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab247e6-a69e-439a-88c2-531e8e39b6af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
