{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cba4324-6733-4e5b-a34a-33a68c18f9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd\n",
    "%cd /app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2499f5-ee6b-4853-acb6-cceee542feaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dac\n",
    "from audiotools import AudioSignal\n",
    "\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0497f7-32d0-4aeb-89f0-af8c8da9455e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import soundfile as sf\n",
    "import IPython.display as ipd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e518f9-5629-4ab2-84c7-5506579d5a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.device_count()\n",
    "torch.cuda.get_device_properties(0).total_memory/1e9\n",
    "torch.cuda.get_device_properties(1).total_memory/1e9\n",
    "device = torch.device(\"cuda:0\") # if the docker was started with --gpus all, then can choose here.\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93df1cca-b684-46e7-9d18-8c019034da9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = dac.utils.download(model_type=\"16khz\") \n",
    "wavefilepath=\"/scratch/dacdevdata/16kHz/N12/PisWinAppBee_sparse_recon/\"\n",
    "# !ls {datadir}\n",
    "\n",
    "sample_length=16000  # will shorten input files to save memory\n",
    "\n",
    "\n",
    "### This model doesn't sound as good - because it was trained on different data???\n",
    "#model_path = \"/scratch/codecs/codec.pth\" # /the default model from vampnet!\n",
    "model = dac.DAC.load(model_path)\n",
    "model.to(device); #wanna see the model? remove the semicolon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d75681-9a6a-49cc-8620-e838ca16079e",
   "metadata": {},
   "source": [
    "<div style=\"height: 10px; background-color: blue;\"></div>\n",
    "<span style=\"font-size: 24px; color:blue\">Utils </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f90c229-d8ea-491b-aedc-c256c686a54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\"\"\"\n",
    "    Interpolates and interleaves vectors within the input tensor.\n",
    "    Basically \"upsampling\" by inserting interpolated vectors between neighbors in the input.\n",
    "\n",
    "    Args:\n",
    "        input_tensor (torch.Tensor): A PyTorch tensor with shape [1, a, b].\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A tensor with shape [1, a, 2*b - 1], where vectors are interpolated\n",
    "        and interleaved with the original vectors along the last dimension.\n",
    "\n",
    "    Example:\n",
    "        >>> input_tensor = torch.randn(1, 3, 4)\n",
    "        >>> output_tensor = interpolate_and_interleave(input_tensor)\n",
    "\"\"\"\n",
    "def interpolate_and_interleave(input_tensor):\n",
    "    # Get the shape of the input tensor\n",
    "    input_shape = input_tensor.shape\n",
    "\n",
    "    # Compute vectors halfway between neighbors\n",
    "    output_vectors = 0.5 * (input_tensor[:, :, :-1] + input_tensor[:, :, 1:])\n",
    "\n",
    "    # Create a new tensor with the correct shape [1, a, 2*b - 1]\n",
    "    interleaved_tensor = torch.empty((input_shape[0], input_shape[1], 2 * input_shape[2] - 1),\n",
    "                                     dtype=input_tensor.dtype, device=input_tensor.device)\n",
    "\n",
    "    # Copy input vectors and interpolated vectors to the new tensor\n",
    "    interleaved_tensor[:, :, ::2] = input_tensor\n",
    "    interleaved_tensor[:, :, 1::2] = output_vectors\n",
    "\n",
    "    return interleaved_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544582ba-6a27-4930-ba80-3e3856afe83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example\n",
    "input_tensor = torch.randn(1, 1, 4)\n",
    "print(input_tensor)\n",
    "output_tensor = interpolate_and_interleave(input_tensor)\n",
    "print(output_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bebbe5-8a7c-44be-b5e2-e9c5749a6cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####    CHOOSE A SOUND\n",
    "\n",
    "#snd2='/DSApplause--numClappers_exp-00.50--c-01--x-01.wav' \n",
    "#snd2='/DSPistons--rate_exp-00.50--c-01--x-01.wav'\n",
    "snd2='DSBugs--busybodyFreqFactor-00.50--c-01--x-01.wav' #'/DSBugs--busybodyFreqFactor-00.50.wav'\n",
    "#snd2='/DSWind--strength-00.50--c-01--x-01.wav'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e3c1a3-7126-4f6d-a0ea-c9f11e4c553f",
   "metadata": {},
   "source": [
    "<div style=\"height: 10px; background-color: blue;\"></div>\n",
    "<span style=\"font-size: 24px; color:blue\">Load a file and code it</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c164bcd9-af27-4f42-9962-5771a41fd4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first load a wave file, \n",
    "bee = AudioSignal(wavefilepath + snd2) # 2-second sound at 16kHz\n",
    "bee = bee[0,0,: sample_length] # cortado, otherwise the computation will bust memory\n",
    "\n",
    "\n",
    "# #bee_compressed = model.compress(bee)\n",
    "# #print(f'bee_compressed.codes.shape = {bee_compressed.codes.shape}')\n",
    "# #print(bee_compressed)\n",
    "\n",
    "#get the z vectors that we can use to get a \"reconstructed\" version (because we will be also reconstructing from artifical z's later, and want to have the padding, etc all be the same between the original and the artifical sound)\n",
    "bee.to(model.device)\n",
    "bee_x = model.preprocess(bee.audio_data, bee.sample_rate)\n",
    "\n",
    "print(f'bee_x is {bee_x} with shape {bee_x.shape}')\n",
    "\n",
    "########  ENCODE\n",
    "bee_z, bee_codes, bee_latents, _, _ = model.encode(bee_x) # , n_quantizers=1) # model.encode(bee_x, 4)\n",
    "print(f'### ENCODE ---  bee_z is {bee_z} with shape {bee_z.shape}')\n",
    "print(f'### ENCODE ---  bee_codes is {bee_codes} with shape {bee_codes.shape}')\n",
    "print(f'### ENCODE ---  bee_latents is {bee_latents} with shape {bee_latents.shape}')\n",
    "########  DECODE\n",
    "beerecon = model.decode(bee_z) # now we have something that we can use to compare with artificial z vectors\n",
    "print(f'### DECODE ---  beerecon is {beerecon} with shape {beerecon.shape}')\n",
    "\n",
    "\n",
    "# bee.to(model.device)\n",
    "# bee_x = model.preprocess(bee.audio_data, bee.sample_rate)\n",
    "\n",
    "########  ENCODE\n",
    "recon_z, recon_codes, recon_latents, _, _ = model.encode(torch.cat((beerecon, torch.zeros(1, 1, 4, device=device)), dim=2)) # , n_quantizers=1) # model.encode(bee_x, 4)\n",
    "print(f'### ENCODE --- recon_z.shape = {recon_z.shape},  recon_codes.shape = {recon_codes.shape}, recon_latents.shape = {recon_latents.shape}')\n",
    "\n",
    "########  DECODE\n",
    "beerecon2 = model.decode(recon_z) # now we have something that we can use to compare with artificial z vectors\n",
    "print(f'### DECODE ---  beerecon2 is {beerecon2} with shape {beerecon2.shape}')\n",
    "\n",
    "\n",
    "\n",
    "# bee_compressed = model.compress(beerecon)\n",
    "# print(bee_compressed)\n",
    "# bee_compressed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16972498-6aff-4614-8083-5394198e2075",
   "metadata": {},
   "outputs": [],
   "source": [
    "bee.to(model.device)\n",
    "bee_x = model.preprocess(bee.audio_data, bee.sample_rate)\n",
    "bee_z, bee_codes, bee_latents, _, _ = model.encode(bee_x, n_quantizers=1) # model.encode(bee_x, 4)\n",
    "\n",
    "print(f'bee_z.shape = {bee_z.shape}')\n",
    "print(f'bee_z 1 time slize slice = {bee_z[0,:,40]}')\n",
    "print(f'bee_codes.shape = {bee_codes.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c9da79-ee2d-4645-914e-663b8edcd047",
   "metadata": {},
   "outputs": [],
   "source": [
    "beerecon = model.decode(bee_z)\n",
    "print(f'Shape of reconstructed signal is {beerecon.shape}')\n",
    "\n",
    "# beereconsignal = beerecon[0,0,:].cpu().detach().numpy()\n",
    "# plt.plot(beereconsignal)\n",
    "# ipd.Audio(beereconsignal, rate=44100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d060c6-cb70-41a2-8167-5af1b0b726e8",
   "metadata": {},
   "source": [
    "<div style=\"height: 10px; background-color: blue;\"></div>\n",
    "<span style=\"font-size: 24px; color:blue\">Now mess with it and adjust the meta data to reflect the messing</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa0bf57-52e9-4423-ade7-adefa7dbcb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First get tensors and signals we will need:\n",
    "\n",
    "#messmethod=\"INTERPOLATE\"\n",
    "#messmethod=\"CONTRACT\"\n",
    "#messmethod=\"EXPAND\"\n",
    "#messmethod=\"SHIFT\"\n",
    "messmethod=\"RANDPERTURB\"\n",
    "#messmethod=\"REVERSE\"\n",
    "\n",
    "if messmethod ==\"INTERPOLATE\" :\n",
    "    sloebee_z = interpolate_and_interleave(bee_z)\n",
    "    sloebee_z = sloebee_z[:,:, : int(bee_z.shape[2])] #run out of memory if too long, so shorten to original sound length\n",
    "elif messmethod==\"CONTRACT\" :\n",
    "     sloebee_z = .15*bee_z\n",
    "elif messmethod==\"EXPAND\" :\n",
    "     sloebee_z = 1.8*bee_z\n",
    "elif messmethod==\"SHIFT\" :\n",
    "     sloebee_z = bee_z+10\n",
    "elif messmethod==\"RANDPERTURB\" :\n",
    "     sloebee_z = bee_z+30*torch.rand_like(bee_z)\n",
    "elif messmethod==\"REVERSE\" :\n",
    "     sloebee_z = torch.flip(bee_z, dims=(2,))\n",
    "     #sloebee_z = bee_z\n",
    "    \n",
    "sloebeerecon = model.decode(sloebee_z)\n",
    "print(f'sloebeerecon.shape is {sloebeerecon.shape}..........bee.shape is {bee.shape}')\n",
    "sloebee=AudioSignal(sloebeerecon, sample_rate=16000)\n",
    "sloebee_compressed = model.compress(sloebee)\n",
    "sloebee_compressed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f762c62-97b0-41de-8151-9cf505ec1d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#-----------------------------------------------------------\n",
    "#A little analysis:\n",
    "print(f'sloebee_z.shape = {sloebee_z.shape}')\n",
    "print(f'bee_z 1 time slize slice = {bee_z[0,:,40]}')\n",
    "print(f'sloebee_z 1 time slize slice = {sloebee_z[0,:,40]}')\n",
    "\n",
    "l2_norm = torch.norm(bee_z - sloebee_z, 2)\n",
    "print(\"L2 Norm:\", l2_norm.item())\n",
    "\n",
    "\n",
    "# print(f'bee_compressed.codes.shape = {bee_compressed.codes.shape}')\n",
    "# print(f'sloebee_compressed.codes.shape = {sloebee_compressed.codes.shape}')\n",
    "\n",
    "# numcodebooks=1\n",
    "# t1=bee_compressed.codes[0,numcodebooks-1,:]\n",
    "# t2=sloebee_compressed.codes[0,numcodebooks-1,:]\n",
    "# same_elements = (t1 == t2).sum().item()\n",
    "# print(\"Ratio of codes that are the same in the {t1.numel()} elements that make up the first {numcodebooks} codeboods is \", same_elements/t1.numel())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2636a173-ec8e-43e9-be77-d20875dfcde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sloebeereconsignal = sloebeerecon[0,0,:].cpu().detach().numpy()\n",
    "\n",
    "plt.plot(sloebeereconsignal)\n",
    "ipd.Audio(sloebeereconsignal, rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bc4000-4e32-4b7a-af85-ad8852d342e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1560f9-4e3f-47da-94de-3cfe8cbea393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_dac_file = dac.DACFile(\n",
    "#         codes=sloebee_z,\n",
    "#         chunk_length=bee_compressed.chunk_length,\n",
    "#         original_length=2*bee_compressed.original_length,\n",
    "#         input_db=bee_compressed.input_db,\n",
    "#         channels=bee_compressed.channels,\n",
    "#         sample_rate=bee_compressed.sample_rate,\n",
    "#         padding=bee_compressed.padding,\n",
    "#         dac_version=bee_compressed.dac_version,\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e31c2f-ecf0-47cc-8bb4-fe2721ab868c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
